{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Machine Learning - Stanford\n",
    "__Andrew NG - Coursera__\n",
    "\n",
    "Notes by: David Faria\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "***\n",
    "\n",
    "## Regression\n",
    "+ Examples:\n",
    "    + House Pricing\n",
    "        \n",
    "Fit a function (courbe) to data that contains \"right (or labeled) answer\" in order to produce more, non existing answer\n",
    "\n",
    "###  Linear Regression\n",
    "\n",
    "In linear regression, the hypothesis is a straight line. An example for univariate linear regression:\n",
    "\n",
    "$$h_\\theta(x)=\\theta_0 + \\theta_1x$$\n",
    "\n",
    "The ideal is to find the values of $\\theta_0$ and $\\theta_1$ so the error between our hypothesis and the real value is as smallest as possible:\n",
    "\n",
    "$$error = (h_\\theta(x^ {(i)})-y^{(i)})^2)$$\n",
    "\n",
    "##### Cost Function\n",
    "We define therefore a cost function $J$ over this error: \n",
    "\n",
    "$$J(\\theta_0,\\theta_1) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_\\theta(x^ {(i)})-y^{(i)})^2)$$\n",
    "\n",
    "where $m$ is the number of observations, and the idea is to find $min(J(\\theta_0,\\theta_1))$. Gradient Descent is an algorithm that allows to do so\n",
    "\n",
    "##### Gradient Descent\n",
    "For some function $J(\\theta_0,\\theta_1)$, we start with some $\\theta_0$, $\\theta_1$, and then keep changing $\\theta_0$, $\\theta_1$ to reduce $J(\\theta_0,\\theta_1)$ until hopefully end up at a minimum.\n",
    "\n",
    "To do so we repeat until convergence:\n",
    "$$\n",
    "\\theta_j:=\\theta_j-\\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)\n",
    "$$\n",
    "(for j=0 and j=1).\n",
    "\n",
    "For each step, we update all $\\theta_j$ **before** simultaneously, before replacing the new values in the formula.\n",
    "\n",
    "> **$\\alpha$ is the learning rate** It defines how big the change steps are. In the linear regression case, it is the slope of the cost function. As we approach the local minimum, the term $\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)$ gets closer to 0, which makes every step smaller and smaller. This is why there is no need to decrease $\\alpha$ over time.\n",
    "\n",
    "Working the derivative of the gradient descent equation gives:\n",
    "\n",
    "$$\n",
    "\\theta_0 := \\theta_0 - \\frac{\\alpha}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})\n",
    "$$\n",
    "$$\n",
    "\\theta_1 := \\theta_1 - \\frac{\\alpha}{m}\\sum_{i=1}^{m}(h_\\theta(x^{(i)})-y^{(i)})x^{(i)}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Estimate the probability that an unknown answer is of a certain class (2 to N), based on the features of observations of right (labeled) answers\n",
    "\n",
    "\n",
    "_The objective in machine learning is to be able to use N features automatically_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "Find structure in the data when no information about the right/wrong answers is available. \n",
    "\n",
    "+ **Clustering**\n",
    "    + Examples:\n",
    "        + The Cocktail Party Problem: separate two audio sources from the same recording\n",
    "        + News clustering\n",
    "        + Group individuals according to their genes\n",
    "        + Organize Computing clusters\n",
    "        + Market segmentation\n",
    "        + ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
