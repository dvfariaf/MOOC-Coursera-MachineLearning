
***
# Machine Learning - Stanford
__Andrew NG - Coursera__

Notes by: David Faria
***

# Supervised Learning
***

## Regression
+ Examples:
    + House Pricing
        
Fit a function (courbe) to data that contains "right (or labeled) answer" in order to produce more, non existing answer

###  Linear Regression

In linear regression, the hypothesis is a straight line. An example for univariate linear regression:

$$h_\theta(x)=\theta_0 + \theta_1x$$

The ideal is to find the values of $\theta_0$ and $\theta_1$ so the error between our hypothesis and the real value is as smallest as possible:

$$error = (h_\theta(x^ {(i)})-y^{(i)})^2)$$

##### Cost Function
We define therefore a cost function $J$ over this error: 

$$J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^ {(i)})-y^{(i)})^2)$$

where $m$ is the number of observations, and the idea is to find $min(J(\theta_0,\theta_1))$. Gradient Descent is an algorithm that allows to do so

##### Gradient Descent
For some function $J(\theta_0,\theta_1)$, we start with some $\theta_0$, $\theta_1$, and then keep changing $\theta_0$, $\theta_1$ to reduce $J(\theta_0,\theta_1)$ until hopefully end up at a minimum.

To do so we repeat until convergence:
$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)
$$
(for j=0 and j=1).

For each step, we update all $\theta_j$ **before** simultaneously, before replacing the new values in the formula.

> **$\alpha$ is the learning rate** It defines how big the change steps are. In the linear regression case, it is the slope of the cost function. As we approach the local minimum, the term $\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)$ gets closer to 0, which makes every step smaller and smaller. This is why there is no need to decrease $\alpha$ over time.

Working the derivative of the gradient descent equation gives:

$$
\theta_0 := \theta_0 - \frac{\alpha}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})
$$
$$
\theta_1 := \theta_1 - \frac{\alpha}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}
$$




## Classification

Estimate the probability that an unknown answer is of a certain class (2 to N), based on the features of observations of right (labeled) answers


_The objective in machine learning is to be able to use N features automatically_

## Unsupervised Learning

Find structure in the data when no information about the right/wrong answers is available. 

+ **Clustering**
    + Examples:
        + The Cocktail Party Problem: separate two audio sources from the same recording
        + News clustering
        + Group individuals according to their genes
        + Organize Computing clusters
        + Market segmentation
        + ...
